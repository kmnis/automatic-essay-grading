{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c939ea5",
   "metadata": {},
   "source": [
    "This project is implemented mainly into three phases described as follows:\n",
    "1. PHASE-1: PRE-PROCESSING & EDA\n",
    "   \n",
    "2. PHASE-2: FEATURE EXTRACTION\n",
    "\n",
    "3. PHASE-3: MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c8dd3",
   "metadata": {},
   "source": [
    "## PHASE-1: PRE-PROCESSING & EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22cf3e",
   "metadata": {},
   "source": [
    "This phase is further divided into following three parts:\n",
    "1. DATA CLEANING\n",
    "2. DATA VISUALIZATION\n",
    "3. INSIGHTS DISCOVERY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf4d70d",
   "metadata": {},
   "source": [
    "#### Importing required libraries/packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "656c4294",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-04T06:52:04.194648Z",
     "start_time": "2023-03-04T06:52:03.186035Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "import string\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import contractions\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "\n",
    "# initialize WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create a spellchecker object for English\n",
    "spell = SpellChecker(language='en')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a44094",
   "metadata": {},
   "source": [
    "### 1.A DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976b8589",
   "metadata": {},
   "source": [
    "List of functions created to implement Data Cleaning:\n",
    "1. text_cleaning(text)\n",
    "2. words_correction(word_list)\n",
    "3. lemmatize_with_pos(word)\n",
    "4. count_spelling_mistakes(essay)\n",
    "5. count_pos_tags(tokens)\n",
    "6. assign_score_category(row)\n",
    "7. process_df(df_train, output_csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccf5d74",
   "metadata": {},
   "source": [
    "Let's look at each of them in detail as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcacec4",
   "metadata": {},
   "source": [
    "#### 1. text_cleaning(text):\n",
    "\n",
    "    INPUT: raw essay  \n",
    "    OUTPUT: a list of cleaned tokens (words) \n",
    "\n",
    "    This function is performing following operations:\n",
    "    1. Contractions - to expand the shortened words\n",
    "    2. Tokenization - to convert text in a list of words\n",
    "    3. Cleaning - to remove whitespaces, new lines, tabs, stopwords and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7fda519",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-04T06:52:04.202644Z",
     "start_time": "2023-03-04T06:52:04.196533Z"
    }
   },
   "outputs": [],
   "source": [
    "contractions.contractions_dict['dont'] = 'do not'\n",
    "contractions.contractions_dict['didnt'] = 'did not'\n",
    "contractions.contractions_dict['couldnt'] = 'could not'\n",
    "contractions.contractions_dict['cant'] = 'can not'\n",
    "contractions.contractions_dict['doesnt'] = 'does not'\n",
    "contractions.contractions_dict['wont'] = 'would not'\n",
    "contractions.contractions_dict['shouldnt'] = 'should not'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7077d9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-04T06:52:04.207254Z",
     "start_time": "2023-03-04T06:52:04.203889Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def text_cleaning(text):  \n",
    "\n",
    "    # creating an empty list\n",
    "    expanded_words = [] \n",
    "    \n",
    "    # Perform contractions to convert words like don't to do not\n",
    "    for word in text.split():\n",
    "      # using contractions.fix to expand the shortened words\n",
    "      expanded_words.append(contractions.fix(word))\n",
    "    \n",
    "    expanded_text = ' '.join(expanded_words)\n",
    "    \n",
    "    # tokenizing text \n",
    "    tokens = word_tokenize(expanded_text)\n",
    "    \n",
    "    # converting list to string\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    # convert text to lowercase and remove leading/trailing white space\n",
    "    text = ''.join(text.lower().strip()) \n",
    "    \n",
    "    # remove newlines, tabs, and extra white spaces\n",
    "    text = re.sub('\\n|\\r|\\t', ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = ''.join(text.strip()) \n",
    "\n",
    "    # remove stop words and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    cleaned_text = ''.join([char for char in cleaned_text if char not in string.punctuation])\n",
    "    cleaned_text = ' '.join([char for char in cleaned_text.split() if len(char) > 2]) # Added this for only keeping words with lengths>2\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc62bc7b",
   "metadata": {},
   "source": [
    "#### 2. lemmatize_with_pos(word):\n",
    "\n",
    "    INPUT: list of corrected spelled words  \n",
    "    OUTPUT: list of words in base form \n",
    "\n",
    "    This functions removes stem from words using part-of-speech tagging. It determines the appropriate POS tag for each word using the `get_wordnet_pos()` function, which maps the POS tag to the first character used by the WordNetLemmatizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b498da20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-04T06:52:04.213863Z",
     "start_time": "2023-03-04T06:52:04.209323Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to apply lemmatization with POS tagging to each word\n",
    "def lemmatize_with_pos(word):\n",
    "    pos = get_wordnet_pos(word)\n",
    "    if pos:\n",
    "        return lemmatizer.lemmatize(word, pos=pos)\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word)\n",
    "\n",
    "# define a function to get the appropriate POS tag for a word\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # default to noun if not found\n",
    "\n",
    "# define a function to apply lemmatization to each word\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatize_with_pos(word) for word in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f16881",
   "metadata": {},
   "source": [
    "#### 3. words_correction(word_list):\n",
    "\n",
    "    INPUT: cleaned tokenized list of words  \n",
    "    OUTPUT: list of correct spelled words\n",
    "\n",
    "    If the word is misspelled, then the corrected version of that specific word is added the list of corrected words; else word is directly added to corrected words list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4044e5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-04T06:52:04.218931Z",
     "start_time": "2023-03-04T06:52:04.215459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to count the number of spelling mistakes in a given essay\n",
    "def count_spelling_mistakes(df):\n",
    "    for j in tqdm(range(len(df))):\n",
    "        essay = df.cleaned_tokenize_text[j].split()\n",
    "        \n",
    "        misspelled = list(spell.unknown(essay))\n",
    "        \n",
    "        mistakes, corrections = [], []\n",
    "        for i, m in enumerate(misspelled):\n",
    "            s = spell.correction(m)\n",
    "            if s:\n",
    "                corrections.append(s)\n",
    "                mistakes.append(m)\n",
    "        \n",
    "        for i, m in enumerate(mistakes):\n",
    "            indexes = [i for i, j in enumerate(essay) if j == m]\n",
    "            for ind in indexes:\n",
    "                essay[ind] = corrections[i]\n",
    "\n",
    "        essay = \" \".join(essay)\n",
    "        n_mistakes = len(mistakes)\n",
    "        mistakes = \" \".join(mistakes)\n",
    "        corrections = \" \".join(corrections)\n",
    "\n",
    "        df.loc[j, ['corrected_text', 'mistakes', 'corrected_words', 'num_mistakes']] = [essay, mistakes, corrections, n_mistakes]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a76ad",
   "metadata": {},
   "source": [
    "#### 4. count_pos_tags(tokens):\n",
    "\n",
    "    INPUT: list of words in base form (lemmatized words)  \n",
    "    OUTPUT: number of nouns, verbs, adverbs and adjectives in a essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c030c95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-04T06:52:04.223729Z",
     "start_time": "2023-03-04T06:52:04.220886Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_pos_tags(tokens):\n",
    "    noun_count = 0\n",
    "    verb_count = 0\n",
    "    adjective_count = 0\n",
    "    adverb_count = 0\n",
    "    \n",
    "    # loop through each token and increment the corresponding counter\n",
    "    for token, tag in nltk.pos_tag(tokens):\n",
    "        if tag.startswith('N'):  # noun\n",
    "            noun_count += 1\n",
    "        elif tag.startswith('V'):  # verb\n",
    "            verb_count += 1\n",
    "        elif tag.startswith('J'):  # adjective\n",
    "            adjective_count += 1\n",
    "        elif tag.startswith('R'):  # adverb\n",
    "            adverb_count += 1\n",
    "    \n",
    "    # return a dictionary with the counts\n",
    "    return {'noun': noun_count, 'verb': verb_count, 'adjective': adjective_count, 'adverb': adverb_count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0902d242",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-04T06:52:04.227401Z",
     "start_time": "2023-03-04T06:52:04.225560Z"
    }
   },
   "outputs": [],
   "source": [
    "def list_to_string(lst):\n",
    "    return ' '.join(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560cc43d",
   "metadata": {},
   "source": [
    "#### 5. assign_score_category(row):\n",
    "\n",
    "    INPUT: each row of target features  \n",
    "    OUTPUT: categorical label of low, high, medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "679cd23f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-04T06:52:04.231798Z",
     "start_time": "2023-03-04T06:52:04.229056Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to assign score category based on scores\n",
    "def assign_score_category(row):\n",
    "    if (row[['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']] <= 2.5).sum() > 4:\n",
    "        return 'low'\n",
    "    elif (row[['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']] >= 4).sum() > 4:\n",
    "        return 'high'\n",
    "    else:\n",
    "        return 'medium'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0528d8",
   "metadata": {},
   "source": [
    "#### 6. process_df(df):\n",
    "\n",
    "    INPUT: raw data  \n",
    "    OUTPUT: processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afaf1592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-04T06:52:04.238493Z",
     "start_time": "2023-03-04T06:52:04.233551Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_df(df):\n",
    "\n",
    "    # 1. apply the text_cleaning function to the 'full_text' column using apply() method\n",
    "    print(\"Cleaning essays\")\n",
    "    df['cleaned_tokenize_text'] = df['full_text'].apply(text_cleaning)\n",
    "\n",
    "    # 2. Apply the function to the tokenized text column and store the results in new columns\n",
    "    print(\"Getting count of spelling mistakes\")\n",
    "    df = count_spelling_mistakes(df)\n",
    "    \n",
    "    # 3. apply lemmatize_text function to the corrected_text\n",
    "    print(\"Lemmatizing text\")\n",
    "    df['lemmatized_text'] = df['corrected_text'].apply(lambda x: lemmatize_text(x.split()))\n",
    "    \n",
    "    # 4. Compute the statistics\n",
    "    print(\"Analyzing sentences\")\n",
    "    df['sent_count'] = df['full_text'].apply(lambda x: len(sent_tokenize(x)))\n",
    "\n",
    "    # 5. Compute the average number of words in a sentence in an essay\n",
    "    print(\"Getting average sentence length\")\n",
    "    df['sent_len'] = df['full_text'].apply(lambda x: np.mean([len(w.split()) for w in sent_tokenize(x)]))\n",
    "\n",
    "    # 6. Apply the count_pos_tags function to each row\n",
    "    print(\"POS tagging\")\n",
    "    df['pos_counts'] = df['lemmatized_text'].apply(count_pos_tags)\n",
    "    \n",
    "    # 7. Compute the word count for each essay\n",
    "    df['word_count'] = df.full_text.apply(lambda x: len(x.split()))\n",
    "\n",
    "    # 8. Extract the count for each POS tag into a separate column\n",
    "    print(\"Counting POS tags\")\n",
    "    df['noun_count'] = df['pos_counts'].apply(lambda x: x['noun'])\n",
    "    df['verb_count'] = df['pos_counts'].apply(lambda x: x['verb'])\n",
    "    df['adjective_count'] = df['pos_counts'].apply(lambda x: x['adjective'])\n",
    "    df['adverb_count'] = df['pos_counts'].apply(lambda x: x['adverb'])\n",
    "\n",
    "    # 9. apply the function to create a new column\n",
    "    if \"cohesion\" in df.columns:\n",
    "        df['score_category'] = df.apply(assign_score_category, axis=1)\n",
    "    \n",
    "    # 10. drop the tokens and pos_counts columns\n",
    "    df = df.drop(['pos_counts'], axis=1)\n",
    "    \n",
    "    df['lemmatized_text'] = df['lemmatized_text'].apply(list_to_string)\n",
    "    df['num_mistakes'] = df['num_mistakes'].apply(int)\n",
    "    df['sent_len'] = df['sent_len'].apply(int)\n",
    "    df['sent_count'] = df['sent_count'].apply(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12601fe8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-04T07:00:09.109273Z",
     "start_time": "2023-03-04T06:52:04.240513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning essays\n",
      "Getting count of spelling mistakes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f6d35835494de8a115d881b154a99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3911 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing text\n",
      "Analyzing sentences\n",
      "Getting average sentence length\n",
      "POS tagging\n",
      "Counting POS tags\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/train.csv')\n",
    "df = process_df(df)\n",
    "df.to_csv(\"../data/processed_essays.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
