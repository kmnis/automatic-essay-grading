{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "663277d8",
   "metadata": {},
   "source": [
    "## PHASE-1 PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "656c4294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/ananth/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ananth/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ananth/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/ananth/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/ananth/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ananth/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import string\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import words\n",
    "import contractions\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "\n",
    "# maximum edit distance per dictionary precalculation\n",
    "max_edit_distance_dictionary = 2\n",
    "prefix_length = 7\n",
    "\n",
    "# create objects\n",
    "sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
    "\n",
    "# load dictionary\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# initialize WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create a spellchecker object for English\n",
    "spell = SpellChecker(language='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9624f11",
   "metadata": {},
   "source": [
    "### A. DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7077d9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def text_cleaning(text):  \n",
    "    \n",
    "    # creating an empty list\n",
    "    expanded_words = [] \n",
    "    \n",
    "    #Perform contractions to convert words like don't to do not\n",
    "    for word in text.split():\n",
    "      # using contractions.fix to expand the shortened words\n",
    "      expanded_words.append(contractions.fix(word))\n",
    "    \n",
    "    expanded_text = ' '.join(expanded_words)\n",
    "    \n",
    "    # tokenizing text \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # converting list to string\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    # convert text to lowercase and remove leading/trailing white space\n",
    "    text = ''.join(text.lower().strip()) \n",
    "    \n",
    "    # remove newlines, tabs, and extra white spaces\n",
    "    text = re.sub('\\n|\\r|\\t', ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = ''.join(text.lower().strip()) \n",
    "\n",
    "    # remove stop words and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    cleaned_text = ''.join([char for char in cleaned_text if char not in string.punctuation])\n",
    "    cleaned_text = ' '.join([char for char in cleaned_text.split() if len(char) > 2]) # Added this for only keeping words with lengths>2\n",
    "\n",
    "    cleaned_tokens = cleaned_text.split()\n",
    "    \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e5b7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_correction(word_list):\n",
    "    corrected_words = []\n",
    "    for word in word_list:\n",
    "        # check if word is misspelled\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "        if suggestions:\n",
    "            corrected_word = suggestions[0].term\n",
    "            corrected_words.append(corrected_word)\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return corrected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b498da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to apply lemmatization with POS tagging to each word\n",
    "def lemmatize_with_pos(word):\n",
    "    pos = get_wordnet_pos(word)\n",
    "    if pos:\n",
    "        return lemmatizer.lemmatize(word, pos=pos)\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word)\n",
    "\n",
    "# define a function to get the appropriate POS tag for a word\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # default to noun if not found\n",
    "\n",
    "# define a function to apply lemmatization to each word\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatize_with_pos(word) for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "154cd834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to count the number of spelling mistakes in a given essay\n",
    "def count_spelling_mistakes(essay):\n",
    "    mistakes = []\n",
    "    for word in essay:\n",
    "        if word not in spell and not wn.synsets(word, pos='n'):\n",
    "            mistakes.append(word)\n",
    "    return mistakes, len(mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e88df9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pos_tags(tokens):\n",
    "    noun_count = 0\n",
    "    verb_count = 0\n",
    "    adjective_count = 0\n",
    "    adverb_count = 0\n",
    "    \n",
    "    # loop through each token and increment the corresponding counter\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        if tag.startswith('N'):  # noun\n",
    "            noun_count += 1\n",
    "        elif tag.startswith('V'):  # verb\n",
    "            verb_count += 1\n",
    "        elif tag.startswith('J'):  # adjective\n",
    "            adjective_count += 1\n",
    "        elif tag.startswith('R'):  # adverb\n",
    "            adverb_count += 1\n",
    "    \n",
    "    # return a dictionary with the counts\n",
    "    return {'noun': noun_count, 'verb': verb_count, 'adjective': adjective_count, 'adverb': adverb_count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8317c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = pd.read_csv('../data/train.csv')\n",
    "\n",
    "def process_df(csv_file_path, output_csv_file_path):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df_train = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # 1. apply the text_cleaning function to the 'full_text' column using apply() method\n",
    "    df_train['cleaned_tokenize_text'] = df_train['full_text'].apply(text_cleaning)\n",
    "\n",
    "    # 2. apply word_correction function to the cleaned_tokenize_text\n",
    "    df_train['corrected_text'] = df_train['cleaned_tokenize_text'].apply(lambda x: words_correction(x))\n",
    "\n",
    "    # 3. apply lemmatize_text function to the corrected_text\n",
    "    df_train['lemmatized_text'] = df_train['corrected_text'].apply(lambda x: lemmatize_text(x))\n",
    "\n",
    "    # 4. Compute the statistics\n",
    "    df_train['sent_count'] = df_train['full_text'].apply(lambda x: len(sent_tokenize(x)))\n",
    "\n",
    "    # 5. Compute the average number of words in a sentence in an essay\n",
    "    df_train['sent_len'] = df_train['full_text'].apply(lambda x: np.mean([len(w.split()) for w in sent_tokenize(x)]))\n",
    "\n",
    "    # 6. Apply the function to the tokenized text column and store the results in new columns\n",
    "    df_train[['mistakes', 'num_mistakes']] = df_train['cleaned_tokenize_text'].apply(lambda x: pd.Series(count_spelling_mistakes(x)))\n",
    "\n",
    "    # 7. Apply the count_pos_tags function to each row\n",
    "    df_train['pos_counts'] = df_train['lemmatized_text'].apply(count_pos_tags)\n",
    "\n",
    "    # 8. Extract the count for each POS tag into a separate column\n",
    "    df_train['noun_count'] = df_train['pos_counts'].apply(lambda x: x['noun'])\n",
    "    df_train['verb_count'] = df_train['pos_counts'].apply(lambda x: x['verb'])\n",
    "    df_train['adjective_count'] = df_train['pos_counts'].apply(lambda x: x['adjective'])\n",
    "    df_train['adverb_count'] = df_train['pos_counts'].apply(lambda x: x['adverb'])\n",
    "\n",
    "    # 9. drop the tokens and pos_counts columns\n",
    "    df_train = df_train.drop(['pos_counts'], axis=1)\n",
    "    \n",
    "    # Write the processed data to a CSV file\n",
    "    df_train.to_csv(output_csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c12f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_df(csv_file_path, output_csv_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
