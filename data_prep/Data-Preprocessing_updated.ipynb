{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acafc10a",
   "metadata": {},
   "source": [
    "# AUTOMATIC ESSAY GRADING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23ba34",
   "metadata": {},
   "source": [
    "## MSCA 31008 DATA MINING PRINCIPLES FINAL PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61940ef7",
   "metadata": {},
   "source": [
    "This project is implemented mainly into three phases described as follows:\n",
    "1. PHASE-1: PRE-PROCESSING & EDA\n",
    "   \n",
    "2. PHASE-2: FEATURE EXTRACTION\n",
    "\n",
    "3. PHASE-3: MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663277d8",
   "metadata": {},
   "source": [
    "## PHASE-1: PRE-PROCESSING & EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296a9e69",
   "metadata": {},
   "source": [
    "This phase is further divided into following three parts:\n",
    "1. DATA CLEANING\n",
    "2. DATA VISUALIZATION\n",
    "3. INSIGHTS DISCOVERY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c153bd8d",
   "metadata": {},
   "source": [
    "Importing required libraries/packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "656c4294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'symspellpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontractions\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpkg_resources\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msymspellpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SymSpell, Verbosity\n\u001b[0;32m     25\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maveraged_perceptron_tagger\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'symspellpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import string\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import words\n",
    "import contractions\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "\n",
    "# maximum edit distance per dictionary precalculation\n",
    "max_edit_distance_dictionary = 2\n",
    "prefix_length = 7\n",
    "\n",
    "# create objects\n",
    "sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
    "\n",
    "# load dictionary\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# initialize WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create a spellchecker object for English\n",
    "spell = SpellChecker(language='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9624f11",
   "metadata": {},
   "source": [
    "### 1.A DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da67f838",
   "metadata": {},
   "source": [
    "List of functions created to implement Data Cleaning:\n",
    "1. text_cleaning(text)\n",
    "2. words_correction(word_list)\n",
    "3. lemmatize_with_pos(word)\n",
    "4. count_spelling_mistakes(essay)\n",
    "5. count_pos_tags(tokens)\n",
    "6. assign_score_category(row)\n",
    "7. process_df(df_train, output_csv_file_path)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7660f11b",
   "metadata": {},
   "source": [
    "Let's look at each of them in detail as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320f21e7",
   "metadata": {},
   "source": [
    "1. 'text_cleaning(text)':"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a521274d",
   "metadata": {},
   "source": [
    " INPUT: raw essay  \n",
    "OUTPUT: a list of cleaned tokens (words) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a70c26",
   "metadata": {},
   "source": [
    " \n",
    "This function is performing following operations:\n",
    "1. Contractions - to expand the shortened words\n",
    "2. Tokenization - to convert text in a list of words\n",
    "3. Cleaning - to remove whitespaces, new lines, tabs, stopwords and punctuation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7077d9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def text_cleaning(text):  \n",
    "    \n",
    "    # creating an empty list\n",
    "    expanded_words = [] \n",
    "    \n",
    "    #Perform contractions to convert words like don't to do not\n",
    "    for word in text.split():\n",
    "      # using contractions.fix to expand the shortened words\n",
    "      expanded_words.append(contractions.fix(word))\n",
    "    \n",
    "    expanded_text = ' '.join(expanded_words)\n",
    "    \n",
    "    # tokenizing text \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # converting list to string\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    # convert text to lowercase and remove leading/trailing white space\n",
    "    text = ''.join(text.lower().strip()) \n",
    "    \n",
    "    # remove newlines, tabs, and extra white spaces\n",
    "    text = re.sub('\\n|\\r|\\t', ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = ''.join(text.lower().strip()) \n",
    "\n",
    "    # remove stop words and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    cleaned_text = ''.join([char for char in cleaned_text if char not in string.punctuation])\n",
    "    cleaned_text = ' '.join([char for char in cleaned_text.split() if len(char) > 2]) # Added this for only keeping words with lengths>2\n",
    "\n",
    "    cleaned_tokens = cleaned_text.split()\n",
    "    \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd50af35",
   "metadata": {},
   "source": [
    "2. 'words_correction(word_list)':"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe4e06d",
   "metadata": {},
   "source": [
    " INPUT: cleaned tokenized list of words  \n",
    "OUTPUT: list of correct spelled words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b13f8c",
   "metadata": {},
   "source": [
    "If the word is misspelled, then the corrected version of that specific word is added the list of corrected words; else word is directly added to corrected words list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5b7a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_correction(word_list):\n",
    "    corrected_words = []\n",
    "    for word in word_list:\n",
    "        # check if word is misspelled\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "        if suggestions:\n",
    "            corrected_word = suggestions[0].term\n",
    "            corrected_words.append(corrected_word)\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return corrected_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10839f87",
   "metadata": {},
   "source": [
    "3. 'lemmatize_with_pos(word)':"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366585e9",
   "metadata": {},
   "source": [
    " INPUT: list of corrected spelled words  \n",
    "OUTPUT: list of words in base form "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5eca6b",
   "metadata": {},
   "source": [
    "This functions removes stem from words using part-of-speech tagging. It determines the appropriate POS tag for each word using the 'get_wordnet_pos()' function, which maps the POS tag to the first character used by the WordNetLemmatizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b498da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to apply lemmatization with POS tagging to each word\n",
    "def lemmatize_with_pos(word):\n",
    "    pos = get_wordnet_pos(word)\n",
    "    if pos:\n",
    "        return lemmatizer.lemmatize(word, pos=pos)\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word)\n",
    "\n",
    "# define a function to get the appropriate POS tag for a word\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # default to noun if not found\n",
    "\n",
    "# define a function to apply lemmatization to each word\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatize_with_pos(word) for word in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8481e64",
   "metadata": {},
   "source": [
    "4. 'count_spelling_mistakes(essay)':"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aab887b",
   "metadata": {},
   "source": [
    " INPUT: list of cleaned tokenize words  \n",
    "OUTPUT: number of mistakes and list of words with spelling mistakes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4044e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to count the number of spelling mistakes in a given essay\n",
    "def count_spelling_mistakes(essay):\n",
    "    mistakes = []\n",
    "    for word in essay:\n",
    "        if word not in spell and not wn.synsets(word, pos='n'):`\n",
    "            mistakes.append(word)\n",
    "    return mistakes, len(mistakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab570bf",
   "metadata": {},
   "source": [
    "5. 'count_pos_tags(tokens)':"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f12be2",
   "metadata": {},
   "source": [
    " INPUT: list of words in base form (lemmatized words)  \n",
    "OUTPUT: number of nouns, verbs, adverbs and adjectives in a essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c030c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pos_tags(tokens):\n",
    "    noun_count = 0\n",
    "    verb_count = 0\n",
    "    adjective_count = 0\n",
    "    adverb_count = 0\n",
    "    \n",
    "    # loop through each token and increment the corresponding counter\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        if tag.startswith('N'):  # noun\n",
    "            noun_count += 1\n",
    "        elif tag.startswith('V'):  # verb\n",
    "            verb_count += 1\n",
    "        elif tag.startswith('J'):  # adjective\n",
    "            adjective_count += 1\n",
    "        elif tag.startswith('R'):  # adverb\n",
    "            adverb_count += 1\n",
    "    \n",
    "    # return a dictionary with the counts\n",
    "    return {'noun': noun_count, 'verb': verb_count, 'adjective': adjective_count, 'adverb': adverb_count}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a27e308",
   "metadata": {},
   "source": [
    "6. 'assign_score_category(row)':"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4601e6",
   "metadata": {},
   "source": [
    " INPUT: each row of target features  \n",
    "OUTPUT: categorical label of low, high, medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc0c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to assign score category based on scores\n",
    "def assign_score_category(row):\n",
    "    if all(row[['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']] <= 2.5):\n",
    "        return 'low'\n",
    "    elif all(row[['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']] >= 4):\n",
    "        return 'high'\n",
    "    else:\n",
    "        return 'medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0902d242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_string(lst):\n",
    "    return ' '.join(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe413d",
   "metadata": {},
   "source": [
    "7. 'process_df(df_train, output_csv_file_path)':"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3240e541",
   "metadata": {},
   "source": [
    " INPUT: data and location where processed data can be stored  \n",
    "OUTPUT: processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf1592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df_train, output_csv_file_path):\n",
    "\n",
    "    # 1. apply the text_cleaning function to the 'full_text' column using apply() method\n",
    "    df_train['cleaned_tokenize_text'] = df_train['full_text'].apply(text_cleaning)\n",
    "\n",
    "    # 2. apply word_correction function to the cleaned_tokenize_text\n",
    "    df_train['corrected_text'] = df_train['cleaned_tokenize_text'].apply(lambda x: words_correction(x))\n",
    "\n",
    "    # 3. apply lemmatize_text function to the corrected_text\n",
    "    df_train['lemmatized_text'] = df_train['corrected_text'].apply(lambda x: lemmatize_text(x))\n",
    "\n",
    "    # 4. Compute the statistics\n",
    "    df_train['sent_count'] = df_train['full_text'].apply(lambda x: len(sent_tokenize(x)))\n",
    "\n",
    "    # 5. Compute the average number of words in a sentence in an essay\n",
    "    df_train['sent_len'] = df_train['full_text'].apply(lambda x: np.mean([len(w.split()) for w in sent_tokenize(x)]))\n",
    "\n",
    "    # 6. Apply the function to the tokenized text column and store the results in new columns\n",
    "    df_train[['mistakes', 'num_mistakes']] = df_train['cleaned_tokenize_text'].apply(lambda x: pd.Series(count_spelling_mistakes(x)))\n",
    "\n",
    "    # 7. Apply the count_pos_tags function to each row\n",
    "    df_train['pos_counts'] = df_train['lemmatized_text'].apply(count_pos_tags)\n",
    "    \n",
    "    # Compute the word count for each essay\n",
    "    df_train['word_count'] = df_train.full_text.apply(lambda x: len(x.split()))\n",
    "\n",
    "    # 8. Extract the count for each POS tag into a separate column\n",
    "    df_train['noun_count'] = df_train['pos_counts'].apply(lambda x: x['noun'])\n",
    "    df_train['verb_count'] = df_train['pos_counts'].apply(lambda x: x['verb'])\n",
    "    df_train['adjective_count'] = df_train['pos_counts'].apply(lambda x: x['adjective'])\n",
    "    df_train['adverb_count'] = df_train['pos_counts'].apply(lambda x: x['adverb'])\n",
    "    \n",
    "    # 9. apply the function to create a new column\n",
    "    df_train['Score_Category'] = df_train.apply(assign_score_category, axis=1)\n",
    "\n",
    "    # 10. drop the tokens and pos_counts columns\n",
    "    df_train = df_train.drop(['pos_counts'], axis=1)\n",
    "    \n",
    "    df_train['cleaned_tokenize_text'] = df_train['cleaned_tokenize_text'].apply(list_to_string)\n",
    "    df_train['corrected_text'] = df_train['corrected_text'].apply(list_to_string)\n",
    "    df_train['lemmatized_text'] = df_train['lemmatized_text'].apply(list_to_string)\n",
    "    df_train['mistakes'] = df_train['mistakes'].apply(list_to_string)\n",
    "    \n",
    "    # Write the processed data to a CSV file\n",
    "    df_train.to_csv(output_csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12601fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "output_csv_file_path = ('processed-essay.csv')\n",
    "process_df(df, output_csv_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
